# app/data.py
import io
import re
import time
import math
import json
import asyncio
import logging
from html import escape
from typing import Optional, Dict, Any, Set, List, DefaultDict
from collections import defaultdict
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import urlparse, parse_qs
import aiohttp
import gspread
import pandas as pd
from pandas import DataFrame
from google.oauth2.service_account import Credentials

# ==== –∫–æ–Ω—Ñ–∏–≥ ====
from app.config import (
    SPREADSHEET_URL, CREDS_JSON, SHEET_NAME,
    SCOPES, DATA_TTL, USERS_TTL, TZ_NAME
)

# SEARCH_FIELDS –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤ config, –∏–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç
try:
    from app.config import SEARCH_FIELDS  # type: ignore
except Exception:
    SEARCH_FIELDS = ["—Ç–∏–ø", "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–∫–æ–¥", "oem", "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å"]

logger = logging.getLogger("bot.data")

# ---------------------- –ì–õ–û–ë–ê–õ–¨–ù–´–ï –°–û–°–¢–û–Ø–ù–ò–Ø ----------------
df: Optional[DataFrame] = None
_last_load_ts = 0.0
_search_index: Optional[Dict[str, Set[int]]] = None

# –ö—ç—à –∫–∞—Ä—Ç–∏–Ω–æ–∫: –∫–ª—é—á ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥, –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äî URL
_image_index: Optional[Dict[str, str]] = None

# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–æ–ø—É—Å–∫–∏ (–∏–∑ –ª–∏—Å—Ç–∞ "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏"/"Users")
SHEET_ALLOWED: Set[int] = set()
SHEET_ADMINS: Set[int] = set()
SHEET_BLOCKED: Set[int] = set()
_last_users_ts = 0.0

# –ü–∞–º—è—Ç—å –¥–∏–∞–ª–æ–≥–æ–≤
user_state: Dict[int, Dict[str, Any]] = {}
issue_state: Dict[int, Dict[str, Any]] = {}

# –§–ª–∞–≥–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
_loading_data = False
_loading_users = False

# –°—Ç–µ–π—Ç—ã –¥–∏–∞–ª–æ–≥–∞ —Å–ø–∏—Å–∞–Ω–∏—è (–¥–ª—è ConversationHandler)
ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)

# -------------------------- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–û–ï -----------------
def now_local_str(fmt: str = "%Y-%m-%d %H:%M:%S") -> str:
    try:
        return datetime.now(ZoneInfo(TZ_NAME)).strftime(fmt)
    except Exception:
        return datetime.utcnow().strftime(fmt)

def val(row: dict, key: str, default: str = "‚Äî") -> str:
    v = row.get(key)
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return default
    s = str(v).strip()
    return s if s else default

def format_row(row: dict) -> str:
    return (
        f"üîπ –¢–∏–ø: {val(row, '—Ç–∏–ø')}\n"
        f"üì¶ –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {val(row, '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ')}\n"
        f"üî¢ –ö–æ–¥: {val(row, '–∫–æ–¥')}\n"
        f"üì¶ –ö–æ–ª-–≤–æ: {val(row, '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ')}\n"
        f"üí∞ –¶–µ–Ω–∞: {val(row, '—Ü–µ–Ω–∞')} {val(row, '–≤–∞–ª—é—Ç–∞')}\n"
        f"üè≠ –ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å: {val(row, '–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å')}\n"
        f"‚öôÔ∏è OEM: {val(row, 'oem')}"
    )

def normalize(text: str) -> str:
    return re.sub(r"[^\w\s]", " ", (text or "")).lower().strip()

def squash(text: str) -> str:
    return re.sub(r"[\W_]+", "", (text or "").lower(), flags=re.UNICODE)

# ------------------------- GOOGLE SHEETS ---------------------
def get_gs_client():
    creds_info = json.loads(CREDS_JSON)
    creds = Credentials.from_service_account_info(creds_info, scopes=SCOPES)
    return gspread.authorize(creds)

def _open_data_worksheet(client):
    sh = client.open_by_url(SPREADSHEET_URL)
    if SHEET_NAME:
        try:
            return sh.worksheet(SHEET_NAME)
        except gspread.WorksheetNotFound:
            logger.warning(f"–õ–∏—Å—Ç {SHEET_NAME!r} –Ω–µ –Ω–∞–π–¥–µ–Ω, fallback –Ω–∞ sheet1")
    return sh.sheet1

def _get_all_records_safe(ws) -> list[dict]:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Ç–∞–±–ª–∏—Ü—ã (—É—Å—Ç–æ–π—á–∏–≤ –∫ –¥—É–±–ª—è–º/–ø—É—Å—Ç—ã–º –∑–∞–≥–æ–ª–æ–≤–∫–∞–º).
    """
    values: List[List[Any]] = ws.get_all_values()
    if not values:
        return []
    # –Ω–∞–π–¥—ë–º –ø–µ—Ä–≤—É—é –Ω–µ–ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    header_row_idx = 0
    for i, row in enumerate(values):
        if any(str(c).strip() for c in row):
            header_row_idx = i
            break
    headers_raw = [str(h).strip() for h in values[header_row_idx]]
    # –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—É—Å—Ç ‚Äî –ø–æ–¥—Å—Ç–∞–≤–∏–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä
    headers = []
    used = set()
    for i, h in enumerate(headers_raw):
        name = h or f"__col_{i}"
        # –¥–µ–¥—É–ø
        base = name
        k = 1
        while name.lower() in used:
            k += 1
            name = f"{base}_{k}"
        used.add(name.lower())
        headers.append(name)
    data_rows = values[header_row_idx + 1:]
    out: List[dict] = []
    for r in data_rows:
        # –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        row = list(r) + [""] * (len(headers) - len(r))
        d = {headers[i]: row[i] for i in range(len(headers))}
        out.append(d)
    return out

def load_data_blocking() -> list[dict]:
    client = get_gs_client()
    ws = _open_data_worksheet(client)
    try:
        # –ø—Ä–æ–±—É–µ–º –±—ã—Å—Ç—Ä–æ
        return ws.get_all_records()
    except Exception as e:
        logger.warning(f"get_all_records —É–ø–∞–ª ({e}), –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ _get_all_records_safe()")
        return _get_all_records_safe(ws)

# --------------------- –ü–û–ò–°–ö (–∏–Ω–¥–µ–∫—Å) ------------------------
def build_search_index(df: DataFrame) -> Dict[str, Set[int]]:
    index: DefaultDict[str, Set[int]] = defaultdict(set)
    for col in SEARCH_FIELDS:
        if col not in df.columns:
            continue
        for idx, val in df[col].astype(str).str.lower().items():
            for t in re.findall(r'\w+', val):
                if t:
                    index[t].add(idx)
    return dict(index)

def match_row_by_index(tokens: List[str]) -> Set[int]:
    if not _search_index:
        return set()
    result = None
    for t in tokens:
        indices = _search_index.get(t, set())
        if result is None:
            result = indices.copy()
        else:
            result &= indices
        if not result:
            break
    return result or set()

def _safe_col(df_: DataFrame, col: str) -> Optional[pd.Series]:
    return df_[col].astype(str).str.lower() if col in df_.columns else None

def _relevance_score(row: dict, tokens: List[str], q_squash: str) -> int:
    score = 0
    for f in SEARCH_FIELDS:
        val_ = str(row.get(f, "")).lower()
        if not val_:
            continue
        words = set(re.findall(r'\w+', val_))
        tok_hit = sum(1 for t in tokens if t in words)
        sub_hit = sum(1 for t in tokens if t and t in val_)
        sq = re.sub(r'[\W_]+', '', val_)
        squash_hit = 1 if q_squash and q_squash in sq else 0
        weight = 2 if f in ("–∫–æ–¥", "oem") else 1
        score += weight * (2 * tok_hit + sub_hit) + 3 * squash_hit * weight
    return score

# --------------------- –ö–∞—Ä—Ç–∏–Ω–∫–∏ –ø–æ –ö–û–î–£ -----------------------
def _norm_code(c: str) -> tuple[str, str]:
    raw = (c or "").strip().lower()
    squash_ = re.sub(r'[\W_]+', '', raw, flags=re.UNICODE)
    return raw, squash_

def _filename_from_url(u: str) -> str:
    """
    –î–æ—Å—Ç–∞—ë–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL, –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ ibb/drive.
    """
    try:
        pu = urlparse(u)
        # drive open?id=... ‚Äî –∏–º–µ–Ω–∏ –Ω–µ—Ç, –Ω–æ —á–∞—Å—Ç–æ –µ—Å—Ç—å –≤ og:image, —ç—Ç–æ —É–∂–µ —Ä–∞–∑—Ä—É–ª–∏–º –Ω–∞ —ç—Ç–∞–ø–µ resolve_ibb/drive
        name = pu.path.rsplit("/", 1)[-1]
        if not name or "." not in name:
            # –∏–Ω–æ–≥–¥–∞ –∏–º—è –≤ query (?name=xxx.jpg)
            qs = parse_qs(pu.query)
            for k in ("name", "filename", "file", "img"):
                if k in qs and qs[k]:
                    return qs[k][0]
        return name
    except Exception:
        return ""

def build_image_index(df_: DataFrame) -> Dict[str, str]:
    """
    –ò–Ω–¥–µ–∫—Å –∫–∞—Ä—Ç–∏–Ω–æ–∫ —Å—Ç—Ä–æ–≥–æ –ø–æ –ö–û–î–£:
    - –∫–ª—é—á–∏: raw –∏ squash(–∫–æ–¥)
    - –∑–Ω–∞—á–µ–Ω–∏–µ: URL –∏–∑ —Å—Ç–æ–ª–±—Ü–∞ 'image' (–µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π)
    *–Ω–∏–∫–∞–∫–æ–≥–æ —Ñ–æ–ª–±—ç–∫–∞ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ ‚Äî —ç—Ç–æ —Ç–æ–ª—å–∫–æ –∏–Ω–¥–µ–∫—Å*
    """
    if "image" not in df_.columns:
        return {}
    index: Dict[str, str] = {}
    for _, row in df_.iterrows():
        code_val = str(row.get("–∫–æ–¥", "")).strip()
        url = str(row.get("image", "")).strip()
        if not code_val or not url:
            continue
        raw, sq = _norm_code(code_val)
        if raw and raw not in index:
            index[raw] = url
        if sq and sq not in index:
            index[sq] = url
    return index

async def resolve_ibb_direct_async(url: str) -> str:
    """
    –î–ª—è ibb.co: –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –ø—Ä—è–º–æ–π URL –∫ –∫–∞—Ä—Ç–∏–Ω–∫–µ —á–µ—Ä–µ–∑ og:image
    """
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=12) as resp:
                if resp.status != 200:
                    return url
                html = await resp.text()
        m = re.search(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']', html, re.I)
        return m.group(1) if m else url
    except Exception as e:
        logger.warning(f"resolve_ibb_direct_async fail: {e}")
        return url

def normalize_drive_url(url: str) -> str:
    m = re.search(r'drive\.google\.com/(?:file/d/([-\w]{20,})|open\?id=([-\w]{20,}))', url)
    if m:
        file_id = m.group(1) or m.group(2)
        return f'https://drive.google.com/uc?export=download&id={file_id}'
    return url

async def resolve_image_url_async(u: str) -> str:
    u = (u or "").strip()
    if not u:
        return u
    if "drive.google.com" in u:
        return normalize_drive_url(u)
    if re.match(r"^https?://(www\.)?ibb\.co/", u, re.I):
        return await resolve_ibb_direct_async(u)
    return u

async def find_image_by_code_async(code: str) -> str:
    """
    1) –ò—â–µ–º –≤ _image_index –ø–æ raw/squash(–∫–æ–¥).
    2) –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –ø—Ä–æ–±–µ–≥–∞–µ–º –ø–æ –≤—Å–µ–º image, –∏—â–µ–º –∫–æ–¥ –∫–∞–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫—É –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–±–µ–∑ –∑–Ω–∞–∫–æ–≤).
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ü–£–°–¢–û, –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ (handlers –æ—Ç–ø—Ä–∞–≤—è—Ç –∫–∞—Ä—Ç–æ—á–∫—É –±–µ–∑ —Ñ–æ—Ç–æ).
    """
    if not code:
        return ""
    raw, sq = _norm_code(code)
    if _image_index:
        url = _image_index.get(raw) or _image_index.get(sq)
        if url:
            return url

    # –º–µ–¥–ª–µ–Ω–Ω—ã–π –ø—É—Ç—å: –ø–æ–∏—Å–∫ –≤ df –ø–æ –∏–º–µ–Ω–∏
    global df
    if df is None or "image" not in df.columns:
        return ""
    target = sq
    for _, row in df.iterrows():
        url = str(row.get("image", "")).strip()
        if not url:
            continue
        name = _filename_from_url(url)
        name_sq = squash(name)
        if target and target in name_sq:
            return url
    return ""

async def _download_image_async(url: str) -> Optional[io.BytesIO]:
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=12) as resp:
                if resp.status != 200:
                    return None
                data = await resp.read()
                if len(data) > 5_000_000:
                    return None
                bio = io.BytesIO(data)
                bio.name = "image"
                return bio
    except Exception as e:
        logger.warning(f"Download failed: {e}")
        return None

# ------------------------- –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• -------------------
def initial_load():
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ.
    """
    global df, _last_load_ts, _search_index, _image_index
    data = load_data_blocking()
    new_df = DataFrame(data)
    new_df.columns = new_df.columns.str.strip().str.lower()

    for col in ("–∫–æ–¥", "oem"):
        if col in new_df.columns:
            new_df[col] = new_df[col].astype(str).str.strip().str.lower()
    if "image" in new_df.columns:
        new_df["image"] = new_df["image"].astype(str).str.strip()

    df = new_df
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _last_load_ts = time.time()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ (startup) {len(df)} —Å—Ç—Ä–æ–∫ –∏ –∏–Ω–¥–µ–∫—Å—ã")

    # —Å—Ä–∞–∑—É –ø–æ–¥–≥—Ä—É–∑–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    allowed, admins, blocked = load_users_from_sheet()
    global SHEET_ALLOWED, SHEET_ADMINS, SHEET_BLOCKED, _last_users_ts
    SHEET_ALLOWED, SHEET_ADMINS, SHEET_BLOCKED = allowed, admins, blocked
    _last_users_ts = time.time()
    logger.info(f"üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (startup): allowed={len(allowed)}, admins={len(admins)}, blocked={len(blocked)}")

async def ensure_fresh_data_async(force: bool = False):
    global df, _last_load_ts, _search_index, _image_index, _loading_data
    if not force and df is not None and (time.time() - _last_load_ts <= DATA_TTL):
        return
    if _loading_data:
        return
    _loading_data = True
    try:
        data = await asyncio.to_thread(load_data_blocking)
        new_df = DataFrame(data)
        new_df.columns = new_df.columns.str.strip().str.lower()
        for col in ("–∫–æ–¥", "oem"):
            if col in new_df.columns:
                new_df[col] = new_df[col].astype(str).str.strip().str.lower()
        if "image" in new_df.columns:
            new_df["image"] = new_df["image"].astype(str).str.strip()

        df = new_df
        _search_index = build_search_index(df)
        _image_index = build_image_index(df)
        _last_load_ts = time.time()
        logger.info(f"‚úÖ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏ –∏–Ω–¥–µ–∫—Å—ã")
    finally:
        _loading_data = False

def ensure_fresh_data(force: bool = False):
    if not force and df is not None and (time.time() - _last_load_ts <= DATA_TTL):
        return
    asyncio.create_task(ensure_fresh_data_async(force=True))

# ---------------------- –í–´–ì–†–£–ó–ö–ê (XLSX/CSV) ------------------
def _df_to_xlsx(df_: DataFrame, name: str) -> io.BytesIO:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as w:
        df_.to_excel(w, index=False)
    buf.seek(0)
    buf.name = name
    return buf

# --------------------- –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ò -------------------------
def _truthy(x) -> bool:
    s = str(x).strip().lower()
    return s in {"1", "true", "yes", "y", "–¥–∞", "–∏—Å—Ç–∏–Ω–∞", "ok", "–æ–∫", "allowed", "—Ä–∞–∑—Ä–µ—à–µ–Ω", "—Ä–∞–∑—Ä–µ—à–µ–Ω–æ"} or (s.isdigit() and int(s) > 0)

def _to_int_or_none(x):
    try:
        if x is None or (isinstance(x, float) and pd.isna(x)):
            return None
        s = str(x).strip()
        if not s:
            return None
        m = re.search(r"-?\d+", s)
        return int(m.group(0)) if m else None
    except Exception:
        return None

def _open_users_worksheet(client):
    sh = client.open_by_url(SPREADSHEET_URL)
    try:
        return sh.worksheet("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏")
    except gspread.WorksheetNotFound:
        try:
            return sh.worksheet("Users")
        except gspread.WorksheetNotFound:
            return None

def load_users_from_sheet():
    client = get_gs_client()
    ws = _open_users_worksheet(client)
    if ws is None:
        logger.info("–õ–∏—Å—Ç '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏' –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –¥–æ—Å—Ç—É–ø —Ä–∞–∑—Ä–µ—à—ë–Ω –≤—Å–µ–º.")
        return set(), set(), set()

    # —á–∏—Ç–∞–µ–º —á–µ—Ä–µ–∑ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä (–≤–æ –∏–∑–±–µ–∂–∞–Ω–∏–µ –¥—É–±–ª–µ–π –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
    try:
        rows = _get_all_records_safe(ws)
    except Exception as e:
        logger.warning(f"_get_all_records_safe(users) —É–ø–∞–ª: {e}")
        rows = []
    if not rows:
        logger.info("–õ–∏—Å—Ç '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏' –ø—É—Å—Ç ‚Äî –¥–æ—Å—Ç—É–ø —Ä–∞–∑—Ä–µ—à—ë–Ω –≤—Å–µ–º.")
        return set(), set(), set()

    allowed, admins, blocked = set(), set(), set()

    for row in rows:
        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á–∏
        r = {str(k).strip().lower(): v for k, v in row.items()}

        uid = (
            _to_int_or_none(r.get("user_id"))
            or _to_int_or_none(r.get("userid"))
            or _to_int_or_none(r.get("id"))
            or _to_int_or_none(r.get("uid"))
            or _to_int_or_none(r.get("—Ç–µ–ª–µ–≥—Ä–∞–º id"))
            or _to_int_or_none(r.get("–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"))
        )
        if not uid:
            continue

        role = str(r.get("role") or r.get("—Ä–æ–ª—å") or "").strip().lower()
        is_admin = role in {"admin", "–∞–¥–º–∏–Ω", "administrator", "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä"} or _truthy(r.get("admin"))
        is_allowed = _truthy(r.get("allowed") or r.get("–¥–æ—Å—Ç—É–ø") or (not role or role == "user"))
        is_blocked = _truthy(r.get("blocked") or r.get("ban") or r.get("–∑–∞–ø—Ä–µ—Ç"))

        if is_blocked:
            blocked.add(uid)
        if is_admin:
            admins.add(uid)
            is_allowed = True
        if is_allowed:
            allowed.add(uid)

    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã: allowed={len(allowed)}, admins={len(admins)}, blocked={len(blocked)}")
    return allowed, admins, blocked
