import os
import io
import re
import time
import json
import logging
from typing import Dict, Set, Tuple, List, Optional

import pandas as pd
import aiohttp
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
from zoneinfo import ZoneInfo

logger = logging.getLogger("bot.data")

# ---------- –ö–æ–Ω—Ñ–∏–≥ ----------
try:
    from app.config import (
        SPREADSHEET_URL,
        SAP_SHEET_NAME,          # "SAP"
        USERS_SHEET_NAME,        # "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏" (–æ–ø—Ü.)
        DATA_TTL,                # —Å–µ–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä 600
        SEARCH_COLUMNS,          # ["—Ç–∏–ø","–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ","–∫–æ–¥","oem","–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å"]
    )
except Exception:
    SPREADSHEET_URL = os.getenv("SPREADSHEET_URL", "")
    SAP_SHEET_NAME = os.getenv("SAP_SHEET_NAME", "SAP")
    USERS_SHEET_NAME = os.getenv("USERS_SHEET_NAME", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏")
    DATA_TTL = int(os.getenv("DATA_TTL", "600"))
    SEARCH_COLUMNS = ["—Ç–∏–ø","–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ","–∫–æ–¥","oem","–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å","–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä","oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä"]

GOOGLE_APPLICATION_CREDENTIALS_JSON = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON", "")
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# ---------- –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ----------
df: Optional[pd.DataFrame] = None
_last_load_ts: float = 0.0

_search_index: Dict[str, Set[int]] = {}
_image_index: Dict[str, str] = {}

# –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ handlers)
user_state: Dict[int, dict] = {}
issue_state: Dict[int, dict] = {}

# –°–ø–∏—Å–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
SHEET_ALLOWED: Set[int] = set()
SHEET_ADMINS: Set[int] = set()
SHEET_BLOCKED: Set[int] = set()

# –®–∞–≥–∏ –¥–∏–∞–ª–æ–≥–∞
ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)

# ---------- –£—Ç–∏–ª–∏—Ç—ã ----------
def _norm_code(x: str) -> str:
    return re.sub(r"[\s\-]+", "", str(x or "").strip().lower())

def _norm_str(x: str) -> str:
    return str(x or "").strip().lower()

def now_local_str(tz_name: str = "Asia/Tashkent") -> str:
    tz = ZoneInfo(tz_name)
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

def val(d: dict, key: str, default: str = "") -> str:
    return str(d.get(key, default) or default)

def _url_name_tokens(url: str) -> List[str]:
    try:
        path = re.sub(r"[?#].*$", "", str(url or ""))
        name = path.rsplit("/", 1)[-1].rsplit(".", 1)[0].lower()
        return re.findall(r"[a-z0-9]+", name)
    except Exception:
        return []

def squash(text: str) -> str:
    return re.sub(r"[\W_]+", "", str(text or "").lower())

def normalize(text: str) -> str:
    return re.sub(r"[^\w\s]", "", str(text or "").lower()).strip()

# ---------- –§–æ—Ä–º–∞—Ç –∫–∞—Ä—Ç–æ—á–∫–∏ ----------
def format_row(row: dict) -> str:
    return (
        f"üîπ –¢–∏–ø: {val(row, '—Ç–∏–ø')}\n"
        f"üì¶ –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {val(row, '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ')}\n"
        f"üî¢ –ö–æ–¥: {val(row, '–∫–æ–¥')}\n"
        f"üî¢ –ü–∞—Ä—Ç –ù–æ–º–µ—Ä: {val(row, '–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä')}\n"
        f"‚öôÔ∏è OEM –ü–∞—Ä—Ç –ù–æ–º–µ—Ä: {val(row, 'oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä')}\n"
        f"üì¶ –ö–æ–ª-–≤–æ: {val(row, '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ')}\n"
        f"üí∞ –¶–µ–Ω–∞: {val(row, '—Ü–µ–Ω–∞')} {val(row, '–≤–∞–ª—é—Ç–∞')}\n"
        f"üè≠ –ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å: {val(row, '–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å')}\n"
        f"‚öôÔ∏è OEM: {val(row, 'oem')}"
    )

# ---------- Google Sheets ----------
def get_gs_client():
    if not GOOGLE_APPLICATION_CREDENTIALS_JSON:
        raise RuntimeError("GOOGLE_APPLICATION_CREDENTIALS_JSON –Ω–µ –∑–∞–¥–∞–Ω")
    try:
        info = json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)
        creds = Credentials.from_service_account_info(info, scopes=SCOPES)
    except json.JSONDecodeError:
        creds = Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS_JSON, scopes=SCOPES)
    return gspread.authorize(creds)

def _load_sap_dataframe() -> pd.DataFrame:
    client = get_gs_client()
    sh = client.open_by_url(SPREADSHEET_URL)
    ws = sh.worksheet(SAP_SHEET_NAME)
    records = ws.get_all_records()
    new_df = pd.DataFrame(records)
    new_df.columns = [c.strip().lower() for c in new_df.columns]
    for col in ("–∫–æ–¥", "oem", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä"):
        if col in new_df.columns:
            new_df[col] = new_df[col].astype(str).str.strip().str.lower()
    if "image" in new_df.columns:
        new_df["image"] = new_df["image"].astype(str).str.strip()
    return new_df

# ---------- –ò–Ω–¥–µ–∫—Å—ã ----------
def build_search_index(df_: pd.DataFrame) -> Dict[str, Set[int]]:
    idx: Dict[str, Set[int]] = {}
    cols = [c for c in SEARCH_COLUMNS if c in df_.columns]
    for i, row in df_.iterrows():
        for c in cols:
            tokenized = re.findall(r"[a-zA-Z–∞-—è–ê-–Ø0-9]+", str(row.get(c, "")), flags=re.IGNORECASE)
            for t in tokenized:
                key = _norm_str(t)
                if not key:
                    continue
                idx.setdefault(key, set()).add(i)
    return idx

def build_image_index(df_: pd.DataFrame) -> Dict[str, str]:
    index: Dict[str, str] = {}
    if "image" not in df_.columns:
        return index
    skip = {"png", "jpg", "jpeg", "gif", "webp", "svg"}
    for _, row in df_.iterrows():
        url = str(row.get("image", "")).strip()
        if not url:
            continue
        tokens = _url_name_tokens(url)
        if not tokens:
            continue
        for t in tokens:
            if t in skip or len(t) < 4:
                continue
            index.setdefault(t, url)
        name_join = "".join(tokens)
        index.setdefault(name_join, url)
    return index

def ensure_fresh_data(force: bool = False):
    global df, _search_index, _image_index, _last_load_ts
    need = force or df is None or (time.time() - _last_load_ts > DATA_TTL)
    if not need:
        return
    new_df = _load_sap_dataframe()
    df = new_df
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _last_load_ts = time.time()
    logger.info(f"‚úÖ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∏–Ω–¥–µ–∫—Å—ã")

# ---------- –ü–æ–∏—Å–∫ ----------
def _relevance_score(row: dict, tokens: List[str], q_squash: str) -> float:
    tkns = [_norm_str(t) for t in tokens if t]
    if not tkns:
        return 0.0
    code = _norm_str(row.get("–∫–æ–¥", ""))
    name = _norm_str(row.get("–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", ""))
    weights = {"–∫–æ–¥": 5.0, "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": 3.0}
    fields = {"–∫–æ–¥": code, "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name}
    score = 0.0
    for f, text in fields.items():
        for t in tkns:
            if t and (t in text):
                score += weights[f]
    if q_squash:
        joined = squash(code + name)
        if q_squash in joined:
            score += 10.0
    if code and any(code.startswith(t) for t in tkns):
        score += 20.0
    return score

def match_row_by_index(tokens: List[str]) -> List[int]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã —Å—Ç—Ä–æ–∫, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
    –°–Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ, –ø–æ—Ç–æ–º fallback ‚Äî –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ.
    """
    ensure_fresh_data()
    if not tokens:
        return []
    tokens_norm = [_norm_str(t) for t in tokens if t]
    if not tokens_norm:
        return []
    # —Å—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫
    sets: List[Set[int]] = []
    for t in tokens_norm:
        s = _search_index.get(t, set())
        if not s:
            sets = []
            break
        sets.append(s)
    if sets:
        acc = sets[0].copy()
        for s in sets[1:]:
            acc &= s
            if not acc:
                break
        found = acc
    else:
        # –º—è–≥–∫–∏–π –ø–æ–∏—Å–∫
        found = set()
        for t in tokens_norm:
            found |= _search_index.get(t, set())
    if not found:
        return []
    q_squash = squash(" ".join(tokens_norm))
    scored = [(idx, _relevance_score(df.iloc[idx].to_dict(), tokens_norm, q_squash)) for idx in found]
    scored.sort(key=lambda x: x[1], reverse=True)
    return [idx for idx, _ in scored]

# ---------- –≠–∫—Å–ø–æ—Ä—Ç ----------
def _df_to_xlsx(df_: pd.DataFrame, filename: str = "export.xlsx") -> io.BytesIO:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
        df_.to_excel(writer, index=False)
    buf.seek(0)
    return buf

# ---------- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ ----------
def load_users_from_sheet() -> Tuple[Set[int], Set[int], Set[int]]:
    allowed: Set[int] = set()
    admins: Set[int] = set()
    blocked: Set[int] = set()
    try:
        client = get_gs_client()
        sh = client.open_by_url(SPREADSHEET_URL)
        ws = sh.worksheet(USERS_SHEET_NAME)
    except Exception:
        logger.info("–õ–∏—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return allowed, admins, blocked
    all_vals = ws.get_all_values()
    if not all_vals:
        return allowed, admins, blocked
    headers = [h.strip().lower() for h in all_vals[0]]
    rows = all_vals[1:]
    for r in rows:
        row = {headers[i]: (r[i] if i < len(r) else "") for i in range(len(headers))}
        uid = row.get("user_id") or row.get("id") or row.get("uid")
        try:
            uid = int(uid)
        except Exception:
            continue
        role = str(row.get("role") or "").strip().lower()
        if role in ("admin", "–∞–¥–º–∏–Ω"):
            admins.add(uid); allowed.add(uid)
        elif role in ("blocked", "ban"):
            blocked.add(uid)
        else:
            allowed.add(uid)
    return allowed, admins, blocked

# ---------- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ----------
def initial_load():
    ensure_fresh_data(force=True)
    allowed, admins, blocked = load_users_from_sheet()
    SHEET_ALLOWED.clear(); SHEET_ALLOWED.update(allowed)
    SHEET_ADMINS.clear(); SHEET_ADMINS.update(admins)
    SHEET_BLOCKED.clear(); SHEET_BLOCKED.update(blocked)
    logger.info(f"Users: allowed={len(allowed)}, admins={len(admins)}, blocked={len(blocked)}")

