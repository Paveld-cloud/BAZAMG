# app/data.py
import os
import io
import re
import time
import json
import math
import logging
from typing import Dict, Set, Tuple, List, Optional

import pandas as pd
import aiohttp
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
from zoneinfo import ZoneInfo

logger = logging.getLogger("bot.data")

# -------- –ö–æ–Ω—Ñ–∏–≥ --------
try:
    from app.config import (
        SPREADSHEET_URL,
        SAP_SHEET_NAME,          # "SAP"
        IMAGES_SHEET_NAME,       # "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è" (–æ–ø—Ü.)
        USERS_SHEET_NAME,        # "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏" (–æ–ø—Ü.)
        DATA_TTL,                # —Å–µ–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä 600
        SEARCH_COLUMNS,          # ["—Ç–∏–ø","–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ","–∫–æ–¥","oem","–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å"]
    )
except Exception:
    SPREADSHEET_URL = os.getenv("SPREADSHEET_URL", "")
    SAP_SHEET_NAME = "SAP"
    IMAGES_SHEET_NAME = "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
    USERS_SHEET_NAME = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏"
    DATA_TTL = int(os.getenv("DATA_TTL", "600"))
    SEARCH_COLUMNS = ["—Ç–∏–ø", "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–∫–æ–¥", "oem", "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å"]

GOOGLE_APPLICATION_CREDENTIALS_JSON = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON", "")
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# -------- –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ --------
df: Optional[pd.DataFrame] = None
_last_load_ts: float = 0.0

_search_index: Dict[str, Set[int]] = {}
_image_index: Dict[str, str] = {}

user_state: Dict[int, dict] = {}
issue_state: Dict[int, dict] = {}

SHEET_ALLOWED: Set[int] = set()
SHEET_ADMINS: Set[int] = set()
SHEET_BLOCKED: Set[int] = set()

# –®–∞–≥–∏ –¥–∏–∞–ª–æ–≥–∞ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ handlers)
ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)

# -------- –£—Ç–∏–ª–∏—Ç—ã --------
def _norm_code(x: str) -> str:
    """–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞: lower + –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤/–¥–µ—Ñ–∏—Å–æ–≤"""
    return re.sub(r"[\s\-]+", "", str(x or "").strip().lower())

def _norm_str(x: str) -> str:
    return str(x or "").strip().lower()

def now_local_str(tz_name: str = "Asia/Tashkent") -> str:
    tz = ZoneInfo(tz_name)
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

def val(d: dict, key: str, default: str = "") -> str:
    return str(d.get(key, default) or default)

def _extract_code_from_url(url: str) -> str:
    """–±–µ—Ä—ë–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–∞–∫ –∫–ª—é—á"""
    try:
        path = re.sub(r"[?#].*$", "", url)
        tail = path.rsplit("/", 1)[-1]
        name = tail.rsplit(".", 1)[0]
        return _norm_code(name)
    except Exception:
        return ""

def _safe_col(df_: pd.DataFrame, col: str) -> Optional[pd.Series]:
    if col not in df_.columns:
        return None
    return df_[col].astype(str).fillna("").str.strip().str.lower()

# -------- –ö–∞—Ä—Ç–æ—á–∫–∞ --------
def format_row(row: dict) -> str:
    return (
        f"üîπ –¢–∏–ø: {val(row, '—Ç–∏–ø')}\n"
        f"üì¶ –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {val(row, '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ')}\n"
        f"üî¢ –ö–æ–¥: {val(row, '–∫–æ–¥')}\n"
        f"üì¶ –ö–æ–ª-–≤–æ: {val(row, '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ')}\n"
        f"üí∞ –¶–µ–Ω–∞: {val(row, '—Ü–µ–Ω–∞')} {val(row, '–≤–∞–ª—é—Ç–∞')}\n"
        f"üè≠ –ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å: {val(row, '–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å')}\n"
        f"‚öôÔ∏è OEM: {val(row, 'oem')}"
    )

# -------- Google Sheets --------
def get_gs_client():
    if not GOOGLE_APPLICATION_CREDENTIALS_JSON:
        raise RuntimeError("GOOGLE_APPLICATION_CREDENTIALS_JSON –Ω–µ –∑–∞–¥–∞–Ω")
    try:
        info = json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)
        creds = Credentials.from_service_account_info(info, scopes=SCOPES)
    except json.JSONDecodeError:
        creds = Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS_JSON, scopes=SCOPES)
    return gspread.authorize(creds)

def _load_sap_dataframe() -> pd.DataFrame:
    client = get_gs_client()
    sh = client.open_by_url(SPREADSHEET_URL)
    ws = sh.worksheet(SAP_SHEET_NAME)
    records = ws.get_all_records()
    new_df = pd.DataFrame(records)

    new_df.columns = [c.strip().lower() for c in new_df.columns]

    for col in ("–∫–æ–¥", "oem"):
        if col in new_df.columns:
            new_df[col] = new_df[col].astype(str).str.strip().str.lower()
    if "image" in new_df.columns:
        new_df["image"] = new_df["image"].astype(str).str.strip()

    return new_df

def _normalize_header_name(h: str, idx: int) -> str:
    """–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Å—Ç–æ–ª–±—Ü–∞ + —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å"""
    name = (h or "").strip().lower()
    name = re.sub(r"[^\w]+", "_", name).strip("_")
    if not name:
        name = f"col{idx+1}"
    return name

def _dedupe_headers(headers: List[str]) -> List[str]:
    seen: Dict[str, int] = {}
    out: List[str] = []
    for i, h in enumerate(headers):
        base = _normalize_header_name(h, i)
        if base not in seen:
            seen[base] = 1
            out.append(base)
        else:
            seen[base] += 1
            out.append(f"{base}_{seen[base]}")
    return out

def _load_images_sheet() -> Dict[str, str]:
    """–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ª–∏—Å—Ç '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è' —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ '–∫–æ–¥','image'"""
    mapping: Dict[str, str] = {}
    try:
        client = get_gs_client()
        sh = client.open_by_url(SPREADSHEET_URL)
        ws = sh.worksheet(IMAGES_SHEET_NAME)
    except Exception:
        return mapping

    # –ß–∏—Ç–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
    all_vals = ws.get_all_values()
    if not all_vals:
        return mapping
    headers_raw = all_vals[0]
    headers = _dedupe_headers(headers_raw)
    rows = all_vals[1:]

    for r in rows:
        row = {headers[i]: (r[i] if i < len(r) else "") for i in range(len(headers))}
        code = _norm_code(row.get("–∫–æ–¥", ""))
        url = str(row.get("image", "")).strip()
        if code and url:
            mapping[code] = url
    logger.info(f"image-index: –∏–∑ –ª–∏—Å—Ç–∞ '{IMAGES_SHEET_NAME}' –ø—Ä–æ—á–∏—Ç–∞–Ω–æ {len(mapping)} —Å—Å—ã–ª–æ–∫")
    return mapping

# -------- –ò–Ω–¥–µ–∫—Å—ã --------
def build_search_index(df_: pd.DataFrame) -> Dict[str, Set[int]]:
    idx: Dict[str, Set[int]] = {}
    cols = [c for c in SEARCH_COLUMNS if c in df_.columns]
    for i, row in df_.iterrows():
        for c in cols:
            tokenized = re.findall(r"[a-zA-Z–∞-—è–ê-–Ø0-9]+", str(row.get(c, "")), flags=re.IGNORECASE)
            for t in tokenized:
                key = _norm_str(t)
                if not key:
                    continue
                idx.setdefault(key, set()).add(i)
    return idx

def build_image_index(df_: pd.DataFrame) -> Dict[str, str]:
    """
    –ò–Ω–¥–µ–∫—Å –∫–∞—Ä—Ç–∏–Ω–æ–∫ ‚Äî –¥–≤–∞ –ø—Ä–æ—Ö–æ–¥–∞ + —Å–ª–∏—è–Ω–∏–µ —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º –ª–∏—Å—Ç–æ–º:
    A) –∫–ª—é—á = –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL (–ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
    B) –∫–ª—é—á = –∑–Ω–∞—á–µ–Ω–∏–µ '–∫–æ–¥' (–µ—Å–ª–∏ –µ—Å—Ç—å image –∏ A –Ω–µ –ø–æ–∫—Ä—ã–ª)
    C) —Å–ª–∏—è–Ω–∏–µ —Å –ª–∏—Å—Ç–æ–º '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è'
    """
    index: Dict[str, str] = {}

    # A: –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    added_a = 0
    if "image" in df_.columns:
        for _, row in df_.iterrows():
            raw_url = str(row.get("image", "")).strip()
            if not raw_url:
                continue
            key = _extract_code_from_url(raw_url)
            if not key:
                continue
            index[key] = raw_url
            added_a += 1

    # B: –ø–æ —Å—Ç–æ–ª–±—Ü—É '–∫–æ–¥'
    added_b = 0
    if "image" in df_.columns and "–∫–æ–¥" in df_.columns:
        for _, row in df_.iterrows():
            raw_url = str(row.get("image", "")).strip()
            if not raw_url:
                continue
            code_val = _norm_code(row.get("–∫–æ–¥", ""))
            if not code_val or code_val in index:
                continue
            index[code_val] = raw_url
            added_b += 1

    # C: –≤–Ω–µ—à–Ω–∏–π –ª–∏—Å—Ç
    ext = _load_images_sheet()
    before = len(index)
    index.update(ext)
    merged = len(index) - before

    logger.info(
        f"image-index: A(filename)={added_a}, B(code-col)={added_b}, "
        f"C(extra-sheet)={merged}, unique_keys={len(index)}"
    )
    return index

def ensure_fresh_data(force: bool = False):
    global df, _search_index, _image_index, _last_load_ts
    need = force or df is None or (time.time() - _last_load_ts > DATA_TTL)
    if not need:
        return
    new_df = _load_sap_dataframe()
    df = new_df
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _last_load_ts = time.time()
    logger.info(f"‚úÖ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∏–Ω–¥–µ–∫—Å—ã")

async def ensure_fresh_data_async(force: bool = False):
    await asyncio_to_thread(ensure_fresh_data, force)

# -------- –ö–∞—Ä—Ç–∏–Ω–∫–∏ --------
async def find_image_by_code_async(code: str) -> str:
    ensure_fresh_data()
    if not code or _image_index is None:
        return ""
    key = _norm_code(code)
    return _image_index.get(key, "")

def normalize_drive_url(url: str) -> str:
    m = re.search(r"drive\.google\.com/(?:file/d/([-\w]{20,})|open\?id=([-\w]{20,}))", url)
    if m:
        file_id = m.group(1) or m.group(2)
        return f"https://drive.google.com/uc?export=download&id={file_id}"
    return url

async def resolve_ibb_direct_async(url: str) -> str:
    try:
        if re.search(r"^https?://i\.ibb\.co/", url, re.I):
            return url
        if not re.search(r"^https?://ibb\.co/", url, re.I):
            return url
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as resp:
                if resp.status != 200:
                    return url
                html = await resp.text()
        m = re.search(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']', html, re.I)
        return m.group(1) if m else url
    except Exception as e:
        logger.warning(f"resolve_ibb_direct_async error: {e}")
        return url

async def resolve_image_url_async(url_raw: str) -> str:
    if not url_raw:
        return ""
    url = normalize_drive_url(url_raw)
    url = await resolve_ibb_direct_async(url)
    return url

# -------- –ü–æ–∏—Å–∫ –∏ —Å–∫–æ—Ä–∏–Ω–≥ --------
def match_row_by_index(tokens: List[str]) -> Set[int]:
    ensure_fresh_data()
    if not tokens:
        return set()
    tokens_norm = [_norm_str(t) for t in tokens if t]
    if not tokens_norm:
        return set()

    sets: List[Set[int]] = []
    for t in tokens_norm:
        s = _search_index.get(t, set())
        if not s:
            return set()
        sets.append(s)

    acc = sets[0].copy()
    for s in sets[1:]:
        acc &= s
        if not acc:
            break
    return acc

def squash(text: str) -> str:
    return re.sub(r"[\W_]+", "", str(text or "").lower())

def normalize(text: str) -> str:
    return re.sub(r"[^\w\s]", "", str(text or "").lower()).strip()

def _relevance_score(row: dict, tokens: List[str], q_squash: str) -> float:
    """
    –ü—Ä–æ—Å—Ç–æ–π —Å–∫–æ—Ä–∏–Ω–≥: –≤–µ—Å–∞ –ø–æ –ø–æ–ª—è–º, –±—É—Å—Ç—ã –∑–∞ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–æ–¥–∞/–Ω–∞—á–∞–ª–æ –∫–æ–¥–∞/—Å–∫–ª–µ–π–∫—É.
    """
    tkns = [_norm_str(t) for t in tokens if t]
    if not tkns:
        return 0.0

    # –ø–æ–ª—è
    code = _norm_str(row.get("–∫–æ–¥", ""))
    name = _norm_str(row.get("–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", ""))
    ttype = _norm_str(row.get("—Ç–∏–ø", ""))
    oem  = _norm_str(row.get("oem", ""))
    manuf= _norm_str(row.get("–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å", ""))

    weights = {
        "–∫–æ–¥": 5.0, "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": 3.0, "—Ç–∏–ø": 2.0, "oem": 2.0, "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å": 2.0
    }
    fields = {
        "–∫–æ–¥": code, "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name, "—Ç–∏–ø": ttype, "oem": oem, "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å": manuf
    }

    score = 0.0
    for f, text in fields.items():
        for t in tkns:
            if not t:
                continue
            if t in text:
                score += weights[f]

    # –±—É—Å—Ç—ã
    if q_squash:
        joined = squash(code + name + ttype + oem + manuf)
        if q_squash in joined:
            score += 10.0

    # —Å–∏–ª—å–Ω—ã–π –±—É—Å—Ç –∑–∞ –∫–æ–¥
    q_full = " ".join(tkns)
    q_full_no_ws = squash(q_full)
    if code:
        if code == q_full:       # —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            score += 100.0
        if code.startswith(q_full) or code.startswith(q_full_no_ws):
            score += 20.0
        # —Ç–∞–∫–∂–µ –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É
        for t in tkns:
            if code.startswith(t):
                score += 5.0

    return score

# -------- –≠–∫—Å–ø–æ—Ä—Ç --------
def _df_to_xlsx(df_: pd.DataFrame, filename: str = "export.xlsx") -> io.BytesIO:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
        df_.to_excel(writer, index=False)
    buf.seek(0)
    return buf

# -------- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ --------
def _parse_int(x) -> Optional[int]:
    try:
        v = int(str(x).strip())
        return v if v > 0 else None
    except Exception:
        return None

def load_users_from_sheet() -> Tuple[Set[int], Set[int], Set[int]]:
    """
    –ë–µ–∑ –ø–∞–¥–µ–Ω–∏—è –Ω–∞ –Ω–µ—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö.
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ö–µ–º:
      - user_id + role (admin|user|blocked)
      - –±—É–ª–µ–≤—ã–µ: allowed/admin/blocked
      - –ø—Ä–æ—Å—Ç–æ —Å–ø–∏—Å–æ–∫ user_id => allowed
    """
    allowed: Set[int] = set()
    admins: Set[int] = set()
    blocked: Set[int] = set()
    try:
        client = get_gs_client()
        sh = client.open_by_url(SPREADSHEET_URL)
        ws = sh.worksheet(USERS_SHEET_NAME)
    except Exception:
        logger.info("–õ–∏—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –ø—É—Å–∫–∞–µ–º –≤—Å–µ—Ö –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        return allowed, admins, blocked

    all_vals = ws.get_all_values()
    if not all_vals:
        logger.info("–õ–∏—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø—É—Å—Ç")
        return allowed, admins, blocked

    headers_raw = all_vals[0]
    headers = _dedupe_headers(headers_raw)
    rows = all_vals[1:]

    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    recs: List[dict] = []
    for r in rows:
        recs.append({headers[i]: (r[i] if i < len(r) else "") for i in range(len(headers))})

    dfu = pd.DataFrame(recs)

    cols = [c.strip().lower() for c in dfu.columns]
    dfu.columns = cols

    has_uid = "user_id" in dfu.columns or "uid" in dfu.columns or "id" in dfu.columns
    has_role = "role" in dfu.columns
    has_allowed = "allowed" in dfu.columns
    has_admin = "admin" in dfu.columns
    has_blocked = "blocked" in dfu.columns

    for _, r in dfu.iterrows():
        uid = _parse_int(r.get("user_id") or r.get("uid") or r.get("id"))
        if not uid:
            continue

        if has_role:
            role = str(r.get("role", "")).strip().lower()
            if role in ("admin", "–∞–¥–º–∏–Ω"):
                admins.add(uid); allowed.add(uid)
            elif role in ("blocked", "ban", "–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω"):
                blocked.add(uid)
            else:
                allowed.add(uid)
            continue

        # –±—É–ª–µ–≤—ã–µ —Ñ–ª–∞–≥–∏
        def truthy(v) -> bool:
            s = str(v).strip().lower()
            return s in ("1", "true", "–¥–∞", "y", "yes")

        if has_blocked and truthy(r.get("blocked")):
            blocked.add(uid); continue
        if has_admin and truthy(r.get("admin")):
            admins.add(uid); allowed.add(uid); continue
        if has_allowed and truthy(r.get("allowed")):
            allowed.add(uid); continue

        # –ø—Ä–æ—Å—Ç–æ —Å–ø–∏—Å–æ–∫
        allowed.add(uid)

    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã: allowed={len(allowed)}, admins={len(admins)}, blocked={len(blocked)}")
    return allowed, admins, blocked

# -------- Async helper --------
import asyncio
async def asyncio_to_thread(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, lambda: func(*args, **kwargs))

# -------- Backward-compat –¥–ª—è main.py --------
def initial_load():
    try:
        ensure_fresh_data(force=True)
    except Exception as e:
        logger.exception(f"initial_load: ensure_fresh_data error: {e}")
        raise
    try:
        allowed, admins, blocked = load_users_from_sheet()
        SHEET_ALLOWED.clear(); SHEET_ALLOWED.update(allowed)
        SHEET_ADMINS.clear(); SHEET_ADMINS.update(admins)
        SHEET_BLOCKED.clear(); SHEET_BLOCKED.update(blocked)
        logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã: allowed={len(allowed)}, admins={len(admins)}, blocked={len(blocked)}")
    except Exception as e:
        logger.warning(f"initial_load: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}")

async def initial_load_async():
    try:
        await ensure_fresh_data_async(force=True)
    except Exception as e:
        logger.exception(f"initial_load_async: ensure_fresh_data_async error: {e}")
        raise
    try:
        allowed, admins, blocked = await asyncio_to_thread(load_users_from_sheet)
        SHEET_ALLOWED.clear(); SHEET_ALLOWED.update(allowed)
        SHEET_ADMINS.clear(); SHEET_ADMINS.update(admins)
        SHEET_BLOCKED.clear(); SHEET_BLOCKED.update(blocked)
        logger.info(f"(async) –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã: allowed={len(allowed)}, admins={len(admins)}, blocked={len(blocked)}")
    except Exception as e:
        logger.warning(f"initial_load_async: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}")

