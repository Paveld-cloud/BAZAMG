#import os
import re
import io
import time
import json
import asyncio
import logging
from typing import Optional, Dict, Any, Set, List, DefaultDict
from collections import defaultdict
from datetime import datetime
from zoneinfo import ZoneInfo

import aiohttp
import gspread
import pandas as pd
from pandas import DataFrame
from google.oauth2.service_account import Credentials

from app.config import (
    TELEGRAM_TOKEN,
    SPREADSHEET_URL,
    GOOGLE_APPLICATION_CREDENTIALS_JSON,
    TZ_NAME,
    SHEET_NAME,
    DATA_TTL,
    USERS_TTL,
    PAGE_SIZE,
    MAX_QTY,
    IMAGE_STRICT,
    SEARCH_FIELDS,
)

logger = logging.getLogger("bot.data")

# ---------------------- –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ----------------------
df: Optional[DataFrame] = None
_last_load_ts = 0.0

_search_index: Optional[Dict[str, Set[int]]] = None
_image_index: Optional[Dict[str, str]] = None  # key: –∫–æ–¥ (lower), val: url

SHEET_ALLOWED: Set[int] = set()
SHEET_ADMINS: Set[int] = set()
SHEET_BLOCKED: Set[int] = set()
_last_users_ts = 0.0

user_state: Dict[int, Dict[str, Any]] = {}
issue_state: Dict[int, Dict[str, Any]] = {}

ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)

# ---------------------- –í—Ä–µ–º—è/—Ñ–æ—Ä–º–∞—Ç ----------------------
def now_local_str(fmt: str = "%Y-%m-%d %H:%M:%S") -> str:
    try:
        return datetime.now(ZoneInfo(TZ_NAME)).strftime(fmt)
    except Exception:
        return datetime.utcnow().strftime(fmt)

# ---------------------- Google Sheets ----------------------
def get_gs_client():
    creds_info = json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)
    creds = Credentials.from_service_account_info(
        creds_info, scopes=["https://www.googleapis.com/auth/spreadsheets"]
    )
    return gspread.authorize(creds)

def _open_data_worksheet(client):
    sh = client.open_by_url(SPREADSHEET_URL)
    if SHEET_NAME:
        try:
            return sh.worksheet(SHEET_NAME)
        except gspread.WorksheetNotFound:
            logger.warning(f"–õ–∏—Å—Ç {SHEET_NAME!r} –Ω–µ –Ω–∞–π–¥–µ–Ω, fallback –Ω–∞ sheet1")
    return sh.sheet1

def load_data_blocking() -> List[dict]:
    client = get_gs_client()
    ws = _open_data_worksheet(client)
    return ws.get_all_records()

# ---------------------- –¢–µ–∫—Å—Ç–æ–≤—ã–µ —É—Ç–∏–ª–∏—Ç—ã ----------------------
def normalize(text: str) -> str:
    return re.sub(r"[^\w\s]", " ", (text or "")).lower().strip()

def squash(text: str) -> str:
    return re.sub(r"[\W_]+", "", (text or "").lower(), flags=re.UNICODE)

def val(row: dict, key: str, default: str = "‚Äî") -> str:
    v = row.get(key)
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return default
    s = str(v).strip()
    return s if s else default

def format_row(row: dict) -> str:
    return (
        f"üîπ –¢–∏–ø: {val(row, '—Ç–∏–ø')}\n"
        f"üì¶ –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {val(row, '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ')}\n"
        f"üî¢ –ö–æ–¥: {val(row, '–∫–æ–¥')}\n"
        f"üì¶ –ö–æ–ª-–≤–æ: {val(row, '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ')}\n"
        f"üí∞ –¶–µ–Ω–∞: {val(row, '—Ü–µ–Ω–∞')} {val(row, '–≤–∞–ª—é—Ç–∞')}\n"
        f"üè≠ –ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å: {val(row, '–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å')}\n"
        f"‚öôÔ∏è OEM: {val(row, 'oem')}"
    )

# ---------------------- –ü–æ–∏—Å–∫ ----------------------
def build_search_index(dataframe: DataFrame) -> Dict[str, Set[int]]:
    index: DefaultDict[str, Set[int]] = defaultdict(set)
    for col in SEARCH_FIELDS:
        if col not in dataframe.columns:
            continue
        series = dataframe[col].astype(str).str.lower()
        for idx, val_ in series.items():
            for t in re.findall(r"\w+", val_):
                if t:
                    index[t].add(idx)
    return dict(index)

def match_row_by_index(tokens: List[str]) -> Set[int]:
    if not _search_index:
        return set()
    result = None
    for t in tokens:
        indices = _search_index.get(t, set())
        if result is None:
            result = indices.copy()
        else:
            result &= indices
        if not result:
            break
    return result or set()

def _safe_col(dataframe: DataFrame, col: str) -> Optional[pd.Series]:
    return dataframe[col].astype(str).str.lower() if col in dataframe.columns else None

def _relevance_score(row: dict, tokens: List[str], q_squash: str) -> int:
    score = 0
    for f in SEARCH_FIELDS:
        val_ = str(row.get(f, "")).lower()
        if not val_:
            continue
        words = set(re.findall(r"\w+", val_))
        tok_hit = sum(1 for t in tokens if t in words)
        sub_hit = sum(1 for t in tokens if t and t in val_)
        sq = re.sub(r"[\W_]+", "", val_)
        squash_hit = 1 if q_squash and q_squash in sq else 0
        weight = 2 if f in ("–∫–æ–¥", "oem") else 1
        score += weight * (2 * tok_hit + sub_hit) + 3 * squash_hit * weight
    return score

# ---------------------- –ö–∞—Ä—Ç–∏–Ω–∫–∏: —Å—Ç—Ä–æ–≥–∏–π –∏–Ω–¥–µ–∫—Å –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ ----------------------
def _extract_code_from_url(url: str) -> str:
    """
    –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø—É—Ç–∏, —É–¥–∞–ª—è–µ–º query/fragment, –æ–±—Ä–µ–∑–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ.
    –ü—Ä–∏–≤–æ–¥–∏–º –∫ lower.
    –ü—Ä–∏–º–µ—Ä—ã:
      https://i.ibb.co/abc/UZ000346.jpg -> uz000346
      https://i.ibb.co/xyz/UZCSS03703.png -> uzcss03703
    """
    try:
        u = (url or "").strip()
        if not u:
            return ""
        # —É–±—Ä–∞—Ç—å query/fragment
        u = u.split("#", 1)[0].split("?", 1)[0]
        # –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø—É—Ç–∏
        last = u.rsplit("/", 1)[-1]
        # –æ—Ç—Ä–µ–∑–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        base = last.split(".", 1)[0]
        return base.strip().lower()
    except Exception:
        return ""

def build_image_index(dataframe: DataFrame) -> Dict[str, str]:
    """
    –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º: –∫–ª—é—á = –∫–æ–¥ –∏–∑ –ò–ú–ï–ù–ò –§–ê–ô–õ–ê (–ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ URL –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è).
    –ï—Å–ª–∏ –∫–æ–¥ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –Ω–µ –∏–∑–≤–ª–µ—á—ë–Ω ‚Äî –∑–∞–ø–∏—Å—å –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è.
    """
    index: Dict[str, str] = {}
    if "image" not in dataframe.columns:
        logger.info("image-index: –∫–æ–ª–æ–Ω–∫–∞ 'image' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return index

    total = len(dataframe)
    added = 0
    for _, row in dataframe.iterrows():
        raw_url = str(row.get("image", "")).strip()
        if not raw_url:
            continue
        code_from_url = _extract_code_from_url(raw_url)
        if not code_from_url:
            continue
        # –ï—Å–ª–∏ —Ö–æ—á–µ—à—å, —á—Ç–æ–±—ã –≤—ã–∏–≥—Ä—ã–≤–∞–ª–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Å—ã–ª–∫–∞ ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ—Ç–∏—Ä–∞–µ–º.
        index[code_from_url] = raw_url
        added += 1

    logger.info(f"image-index[STRICT(FILENAME)]: –¥–æ–±–∞–≤–ª–µ–Ω–æ {added} –∏–∑ {total} —Å—Ç—Ä–æ–∫")
    return index

async def find_image_by_code_async(code: str) -> str:
    """
    –û—Ç–¥–∞—ë–º —Å—Å—ã–ª–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–æ–¥ –Ω–∞–π–¥–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å–µ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
    –ù–∏–∫–∞–∫–∏—Ö —Ñ–æ–ª–±—ç–∫–æ–≤ –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –Ω–µ –¥–µ–ª–∞–µ–º.
    """
    if not code or _image_index is None:
        return ""
    key = (code or "").strip().lower()
    return _image_index.get(key, "")

def normalize_drive_url(url: str) -> str:
    m = re.search(
        r"drive\.google\.com/(?:file/d/([-\w]{20,})|open\?id=([-\w]{20,}))", url
    )
    if m:
        file_id = m.group(1) or m.group(2)
        return f"https://drive.google.com/uc?export=download&id={file_id}"
    return url

async def resolve_ibb_direct_async(url: str) -> str:
    # –ù–∞ –≤—Å—è–∫–∏–π: –µ—Å–ª–∏ –ø—Ä–∏—à–ª—é—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É ibb.co/..., –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –¥–æ—Å—Ç–∞—Ç—å og:image
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=12) as resp:
                if resp.status != 200:
                    return url
                html = await resp.text()
        m = re.search(
            r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
            html,
            re.I,
        )
        return m.group(1) if m else url
    except Exception:
        return url

async def resolve_image_url_async(u: str) -> str:
    u = (u or "").strip()
    if not u:
        return u
    if "drive.google.com" in u:
        return normalize_drive_url(u)
    if re.match(r"^https?://(www\.)?ibb\.co/", u, re.I):
        return await resolve_ibb_direct_async(u)
    return u

# ---------------------- –ó–∞–≥—Ä—É–∑–∫–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ----------------------
async def ensure_fresh_data_async(force: bool = False):
    global df, _last_load_ts, _search_index, _image_index
    if not force and df is not None and (time.time() - _last_load_ts <= DATA_TTL):
        return

    data = await asyncio.to_thread(load_data_blocking)
    new_df = DataFrame(data)
    new_df.columns = new_df.columns.str.strip().str.lower()

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∞–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    for col in ("–∫–æ–¥", "oem"):
        if col in new_df.columns:
            new_df[col] = new_df[col].astype(str).str.strip().str.lower()
    if "image" in new_df.columns:
        new_df["image"] = new_df["image"].astype(str).str.strip()

    df = new_df
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _last_load_ts = time.time()
    logger.info(f"‚úÖ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏ –∏–Ω–¥–µ–∫—Å—ã")

def ensure_fresh_data(force: bool = False):
    if not force and df is not None and (time.time() - _last_load_ts <= DATA_TTL):
        return
    # fire-and-forget
    asyncio.create_task(ensure_fresh_data_async(force=True))

def initial_load():
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞.
    """
    global df, _last_load_ts, _search_index, _image_index
    data = load_data_blocking()
    new_df = DataFrame(data)
    new_df.columns = new_df.columns.str.strip().str.lower()

    for col in ("–∫–æ–¥", "oem"):
        if col in new_df.columns:
            new_df[col] = new_df[col].astype(str).str.strip().str.lower()
    if "image" in new_df.columns:
        new_df["image"] = new_df["image"].astype(str).str.strip()

    df = new_df
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _last_load_ts = time.time()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ (startup) {len(df)} —Å—Ç—Ä–æ–∫ –∏ –∏–Ω–¥–µ–∫—Å—ã")

# ---------------------- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (allowed/admins/blocked) ----------------------
def _truthy(x) -> bool:
    s = str(x).strip().lower()
    return s in {
        "1", "true", "yes", "y", "–¥–∞", "–∏—Å—Ç–∏–Ω–∞", "ok", "–æ–∫",
        "allowed", "—Ä–∞–∑—Ä–µ—à–µ–Ω", "—Ä–∞–∑—Ä–µ—à–µ–Ω–æ"
    } or (s.isdigit() and int(s) > 0)

def _to_int_or_none(x):
    try:
        if x is None or (isinstance(x, float) and pd.isna(x)):
            return None
        s = str(x).strip()
        if not s:
            return None
        m = re.search(r"-?\d+", s)
        return int(m.group(0)) if m else None
    except Exception:
        return None

def load_users_from_sheet():
    """
    –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ª–∏—Å—Ç–∞ '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏' (–∏–ª–∏ 'Users'):
    —á–∏—Ç–∞–µ—Ç –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏,
    –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ –∏ –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏.
    """
    client = get_gs_client()
    sh = client.open_by_url(SPREADSHEET_URL)
    try:
        ws = sh.worksheet("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏")
    except gspread.WorksheetNotFound:
        try:
            ws = sh.worksheet("Users")
        except gspread.WorksheetNotFound:
            logger.info("–õ–∏—Å—Ç '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏' –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –¥–æ—Å—Ç—É–ø —Ä–∞–∑—Ä–µ—à—ë–Ω –≤—Å–µ–º.")
            return set(), set(), set()

    # –≤—Ä—É—á–Ω—É—é —á–∏—Ç–∞–µ–º —Ç–∞–±–ª–∏—Ü—É (–±–µ–∑ get_all_records, —Ç.–∫. —É –Ω–∏—Ö –¥—É–±–ª–∏–∫–∞—Ç—ã —Ö–µ–¥–µ—Ä–æ–≤ —Å–ª—É—á–∞–ª–∏—Å—å)
    all_values = ws.get_all_values()
    if not all_values:
        logger.info("–õ–∏—Å—Ç '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏' –ø—É—Å—Ç ‚Äî –¥–æ—Å—Ç—É–ø —Ä–∞–∑—Ä–µ—à—ë–Ω –≤—Å–µ–º.")
        return set(), set(), set()

    headers = [str(h).strip().lower() for h in (all_values[0] or [])]
    rows = all_values[1:]

    allowed, admins, blocked = set(), set(), set()
    for r in rows:
        row_map = {}
        for i, v in enumerate(r):
            key = headers[i] if i < len(headers) else f"col{i}"
            row_map[key] = v

        # –º–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π
        uid = (
            _to_int_or_none(row_map.get("user_id"))
            or _to_int_or_none(row_map.get("userid"))
            or _to_int_or_none(row_map.get("id"))
            or _to_int_or_none(row_map.get("uid"))
            or _to_int_or_none(row_map.get("—Ç–µ–ª–µ–≥—Ä–∞–º id"))
            or _to_int_or_none(row_map.get("–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"))
        )
        if not uid:
            continue

        role = str(
            row_map.get("role") or row_map.get("—Ä–æ–ª—å") or ""
        ).strip().lower()
        is_admin = role in {"admin", "–∞–¥–º–∏–Ω", "administrator", "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä"} or _truthy(
            row_map.get("admin")
        )
        is_allowed = _truthy(row_map.get("allowed") or row_map.get("–¥–æ—Å—Ç—É–ø") or (not role or role == "user"))
        is_blocked = _truthy(row_map.get("blocked") or row_map.get("ban") or row_map.get("–∑–∞–ø—Ä–µ—Ç"))

        if is_blocked:
            blocked.add(uid)
        if is_admin:
            admins.add(uid)
            is_allowed = True
        if is_allowed:
            allowed.add(uid)

    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã: allowed={len(allowed)}, admins={len(admins)}, blocked={len(blocked)}")
    return allowed, admins, blocked

# ---------------------- –≠–∫—Å–ø–æ—Ä—Ç ----------------------
def _df_to_xlsx(df_: DataFrame, name: str) -> io.BytesIO:
    buf = io.BytesIO()
    # openpyxl –æ–±—è–∑–∞–Ω –±—ã—Ç—å –≤ requirements
    with pd.ExcelWriter(buf, engine="openpyxl") as w:
        df_.to_excel(w, index=False)
    buf.seek(0)
    buf.name = name
    return buf
