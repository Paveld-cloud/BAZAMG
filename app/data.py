# app/data.py
import os
import re
import io
import json
import math
import time
import asyncio
import logging
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import Optional, Dict, Any, Set, List, DefaultDict
from collections import defaultdict

import gspread
import pandas as pd
from pandas import DataFrame
from google.oauth2.service_account import Credentials

from app.config import (
    SPREADSHEET_URL,
    GOOGLE_APPLICATION_CREDENTIALS_JSON,
    SHEET_NAME,
    TZ_NAME,
    DATA_TTL,
    USERS_TTL,
    SEARCH_FIELDS,  # ["—Ç–∏–ø","–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ","–∫–æ–¥","oem","–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å"]
)

logger = logging.getLogger("bot.data")

# ---------------------- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è ----------------------
df: Optional[DataFrame] = None
_last_load_ts = 0.0
_search_index: Optional[Dict[str, Set[int]]] = None
_image_index: Optional[Dict[str, str]] = None

SHEET_ALLOWED: Set[int] = set()
SHEET_ADMINS: Set[int] = set()
SHEET_BLOCKED: Set[int] = set()
_last_users_ts = 0.0

user_state: Dict[int, Dict[str, Any]] = {}
issue_state: Dict[int, Dict[str, Any]] = {}

_loading_data = False
_loading_users = False

# Conversation states (–¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å handlers)
ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)

SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# ---------------------- –í—Ä–µ–º—è ----------------------
def now_local_str(fmt: str = "%Y-%m-%d %H:%M:%S") -> str:
    try:
        return datetime.now(ZoneInfo(TZ_NAME)).strftime(fmt)
    except Exception:
        return datetime.utcnow().strftime(fmt)

# ---------------------- Google Sheets ----------------------
def get_gs_client():
    creds_info = json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)
    creds = Credentials.from_service_account_info(creds_info, scopes=SCOPES)
    return gspread.authorize(creds)

def _open_data_worksheet(client):
    sh = client.open_by_url(SPREADSHEET_URL)
    if SHEET_NAME:
        try:
            return sh.worksheet(SHEET_NAME)
        except gspread.WorksheetNotFound:
            logger.warning(f"–õ–∏—Å—Ç {SHEET_NAME!r} –Ω–µ –Ω–∞–π–¥–µ–Ω, fallback –Ω–∞ sheet1")
    return sh.sheet1

def load_data_blocking() -> list[dict]:
    client = get_gs_client()
    ws = _open_data_worksheet(client)
    return ws.get_all_records()

# ---------------------- –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å ----------------------
def build_search_index(df: DataFrame) -> Dict[str, Set[int]]:
    index: DefaultDict[str, Set[int]] = defaultdict(set)
    for col in SEARCH_FIELDS:
        if col not in df.columns:
            continue
        # .items() —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å (idx, value)
        for idx, val in df[col].astype(str).str.lower().items():
            for t in re.findall(r'\w+', val):
                if t:
                    index[t].add(idx)
    return dict(index)

# ---------------------- –£—Ç–∏–ª–∏—Ç—ã ----------------------
def val(row: dict, key: str, default: str = "‚Äî") -> str:
    v = row.get(key)
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return default
    s = str(v).strip()
    return s if s else default

def format_row(row: dict) -> str:
    return (
        f"üîπ –¢–∏–ø: {val(row, '—Ç–∏–ø')}\n"
        f"üì¶ –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {val(row, '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ')}\n"
        f"üî¢ –ö–æ–¥: {val(row, '–∫–æ–¥')}\n"
        f"üì¶ –ö–æ–ª-–≤–æ: {val(row, '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ')}\n"
        f"üí∞ –¶–µ–Ω–∞: {val(row, '—Ü–µ–Ω–∞')} {val(row, '–≤–∞–ª—é—Ç–∞')}\n"
        f"üè≠ –ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å: {val(row, '–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å')}\n"
        f"‚öôÔ∏è OEM: {val(row, 'oem')}"
    )

def normalize(text: str) -> str:
    return re.sub(r"[^\w\s]", " ", (text or "")).lower().strip()

def squash(text: str) -> str:
    return re.sub(r"[\W_]+", "", (text or "").lower(), flags=re.UNICODE)

def _safe_col(df: DataFrame, col: str) -> Optional[pd.Series]:
    return df[col].astype(str).str.lower() if col in df.columns else None

def _relevance_score(row: dict, tokens: List[str], q_squash: str) -> int:
    score = 0
    for f in SEARCH_FIELDS:
        val_ = str(row.get(f, "")).lower()
        if not val_:
            continue
        words = set(re.findall(r'\w+', val_))
        tok_hit = sum(1 for t in tokens if t in words)
        sub_hit = sum(1 for t in tokens if t and t in val_)
        sq = re.sub(r'[\W_]+', '', val_)
        squash_hit = 1 if q_squash and q_squash in sq else 0
        weight = 2 if f in ("–∫–æ–¥", "oem") else 1
        score += weight * (2 * tok_hit + sub_hit) + 3 * squash_hit * weight
    return score

def match_row_by_index(tokens: List[str]) -> Set[int]:
    if not _search_index:
        return set()
    result = None
    for t in tokens:
        indices = _search_index.get(t, set())
        if result is None:
            result = indices.copy()
        else:
            result &= indices
        if not result:
            break
    return result or set()

# ---------------------- –†–∞–±–æ—Ç–∞ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ ----------------------
def _norm_code(c: str) -> tuple[str, str]:
    raw = (c or "").strip().lower()
    squash_ = re.sub(r'[\W_]+', '', raw, flags=re.UNICODE)
    return raw, squash_

def _url_has_code(url: str, code_raw: str, code_sq: str) -> bool:
    s = (url or "").strip().lower()
    if not s:
        return False
    s_sq = re.sub(r'[\W_]+', '', s, flags=re.UNICODE)
    return (code_raw and code_raw in s) or (code_sq and code_sq in s_sq)

def normalize_drive_url(url: str) -> str:
    m = re.search(r'drive\.google\.com/(?:file/d/([-\w]{20,})|open\?id=([-\w]{20,}))', url)
    if m:
        file_id = m.group(1) or m.group(2)
        return f'https://drive.google.com/uc?export=download&id={file_id}'
    return url

async def resolve_image_url_async(u: str) -> str:
    u = (u or "").strip()
    if not u:
        return u
    if "drive.google.com" in u:
        return normalize_drive_url(u)
    return u

def build_image_index(df: pd.DataFrame) -> dict[str, str]:
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¢–û–õ–¨–ö–û —Ç–µ —Å—Å—ã–ª–∫–∏, –≥–¥–µ –∫–æ–¥ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å–∞–º–æ–º URL (–ø—É—Ç—å/–∫–≤–µ—Ä–∏).
    –ù–∏–∫–∞–∫–∏—Ö —Ñ–æ–ª–±—ç–∫–æ–≤ ¬´–≤–∑—è—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ —Å—Ç—Ä–æ–∫–∏¬ª.
    """
    if "image" not in df.columns or "–∫–æ–¥" not in df.columns:
        return {}

    index: dict[str, str] = {}
    total = 0
    matched = 0
    for _, row in df.iterrows():
        total += 1
        code_val = str(row.get("–∫–æ–¥", "")).strip().lower()
        url = str(row.get("image", "")).strip()
        if not code_val or not url:
            continue

        raw, sq = _norm_code(code_val)
        if _url_has_code(url, raw, sq):
            url_norm = normalize_drive_url(url)
            if raw and raw not in index:
                index[raw] = url_norm
            if sq and sq not in index:
                index[sq] = url_norm
            matched += 1
    logger.info(f"image-index: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ URL={matched} –∏–∑ {total}")
    return index

async def find_image_by_code_async(code: str) -> str:
    if not code or _image_index is None:
        return ""
    raw, sq = _norm_code(code)
    return _image_index.get(raw) or _image_index.get(sq, "") or ""

# ---------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------------------
def initial_load():
    global df, _last_load_ts, _search_index, _image_index
    data = load_data_blocking()
    new_df = DataFrame(data)
    new_df.columns = new_df.columns.str.strip().str.lower()

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–¥/–æ–µ–º –∏ image
    for col in ("–∫–æ–¥", "oem"):
        if col in new_df.columns:
            new_df[col] = new_df[col].astype(str).str.strip().str.lower()
    if "image" in new_df.columns:
        new_df["image"] = new_df["image"].astype(str).str.strip()

    df = new_df
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _last_load_ts = time.time()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ (startup) {len(df)} —Å—Ç—Ä–æ–∫ –∏ –∏–Ω–¥–µ–∫—Å—ã")

    allowed, admins, blocked = load_users_from_sheet()
    global SHEET_ALLOWED, SHEET_ADMINS, SHEET_BLOCKED, _last_users_ts
    SHEET_ALLOWED, SHEET_ADMINS, SHEET_BLOCKED = allowed, admins, blocked
    _last_users_ts = time.time()
    logger.info(f"üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (startup): allowed={len(allowed)}, admins={len(admins)}, blocked={len(blocked)}")

async def ensure_fresh_data_async(force: bool = False):
    global df, _last_load_ts, _search_index, _image_index, _loading_data
    if not force and df is not None and (time.time() - _last_load_ts <= DATA_TTL):
        return
    if _loading_data:
        return
    _loading_data = True
    try:
        data = await asyncio.to_thread(load_data_blocking)
        new_df = DataFrame(data)
        new_df.columns = new_df.columns.str.strip().str.lower()
        for col in ("–∫–æ–¥", "oem"):
            if col in new_df.columns:
                new_df[col] = new_df[col].astype(str).str
